{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Django AI Validator","text":"<p>Django AI Validator is a powerful library that leverages Large Language Models (LLMs) to validate and clean data in your Django applications. It provides seamless integration with Django models, forms, and the admin interface.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Semantic Validation: Validate data based on meaning and context using LLMs (e.g., \"Is this a valid professional bio?\").</li> <li>Automated Cleaning: Automatically normalize and clean data (e.g., removing PII, fixing grammar).</li> <li>Admin Integration: Visual indicators for \"dirty\" data and bulk cleaning actions in Django Admin.</li> <li>Asynchronous Processing: Offload heavy LLM tasks to background workers using Celery.</li> <li>Flexible Providers: Support for OpenAI, Anthropic, and custom LLM providers.</li> <li>Caching: Built-in caching to reduce API costs and latency.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the package via pip:</p> <pre><code>pip install django-ai-validator\n</code></pre> <p>Add it to your <code>INSTALLED_APPS</code>:</p> <pre><code>INSTALLED_APPS = [\n    ...\n    'django_ai_validator',\n]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-configure-api-keys","title":"1. Configure API Keys","text":"<p>Set your API keys in your environment variables:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"#2-add-a-validator","title":"2. Add a Validator","text":"<pre><code>from django.db import models\nfrom django_ai_validator.validators import AISemanticValidator\n\nclass UserProfile(models.Model):\n    bio = models.TextField(\n        validators=[\n            AISemanticValidator(\n                prompt_template=\"Ensure this bio is professional and written in third person.\"\n            )\n        ]\n    )\n</code></pre>"},{"location":"#3-add-a-cleaned-field","title":"3. Add a Cleaned Field","text":"<pre><code>from django_ai_validator.fields import AICleanedField\n\nclass Comment(models.Model):\n    content = AICleanedField(\n        cleaning_prompt=\"Remove any profanity and fix spelling errors.\"\n    )\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#validators","title":"Validators","text":""},{"location":"api/#django_ai_validator.validators","title":"<code>django_ai_validator.validators</code>","text":""},{"location":"api/#django_ai_validator.validators.AISemanticValidator","title":"<code>AISemanticValidator</code>","text":"<p>               Bases: <code>BaseAIValidator</code></p> <p>Concrete implementation of the validator.</p> Source code in <code>src/django_ai_validator/validators.py</code> <pre><code>class AISemanticValidator(BaseAIValidator):\n    \"\"\"\n    Concrete implementation of the validator.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#django_ai_validator.validators.BaseAIValidator","title":"<code>BaseAIValidator</code>","text":"<p>               Bases: <code>BaseValidator</code></p> <p>Template Method Pattern: Defines the skeleton of the validation algorithm.</p> Source code in <code>src/django_ai_validator/validators.py</code> <pre><code>class BaseAIValidator(BaseValidator):\n    \"\"\"\n    Template Method Pattern: Defines the skeleton of the validation algorithm.\n    \"\"\"\n    message = None  # Override BaseValidator's message to avoid limit_value dependency\n\n    def __init__(self, prompt_template, provider=None, message=None, code=None):\n        self.prompt_template = prompt_template\n        self.provider = provider\n        if code:\n            self.code = code\n        # BaseValidator expects a limit_value. We pass None, but then we must ensure\n        # the message doesn't try to use it.\n        super().__init__(limit_value=None, message=message)\n\n    def __call__(self, value):\n        # Template Method\n        if self.should_skip(value):\n            return\n\n        prepared_value = self.prepare_data(value)\n        is_valid, error_reason = self.execute_llm_validation(prepared_value)\n\n        if not is_valid:\n            self.handle_error(value, error_reason)\n\n    def should_skip(self, value):\n        return value in (None, '')\n\n    def prepare_data(self, value):\n        return str(value)\n\n    def execute_llm_validation(self, value):\n        facade = AICleaningFacade(provider=self.provider)\n        return facade.validate(value, self.prompt_template)\n\n    def handle_error(self, value, error_reason):\n        raise ValidationError(\n            self.message or error_reason,\n            code=self.code,\n            params={'value': value},\n        )\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, BaseAIValidator) and\n            self.prompt_template == other.prompt_template and\n            self.message == other.message and\n            self.code == other.code and\n            self.provider == other.provider\n        )\n\n    def deconstruct(self):\n        path = f\"{self.__module__}.{self.__class__.__name__}\"\n        args = [self.prompt_template]\n        kwargs = {}\n        if self.provider:\n            kwargs['provider'] = self.provider\n        if self.message:\n            kwargs['message'] = self.message\n        if self.code:\n            kwargs['code'] = self.code\n        return path, args, kwargs\n</code></pre>"},{"location":"api/#fields","title":"Fields","text":""},{"location":"api/#django_ai_validator.fields","title":"<code>django_ai_validator.fields</code>","text":""},{"location":"api/#admin","title":"Admin","text":""},{"location":"api/#django_ai_validator.admin","title":"<code>django_ai_validator.admin</code>","text":""},{"location":"api/#llm-adapters","title":"LLM Adapters","text":""},{"location":"api/#django_ai_validator.llm.adapters","title":"<code>django_ai_validator.llm.adapters</code>","text":""},{"location":"api/#django_ai_validator.llm.adapters.AnthropicAdapter","title":"<code>AnthropicAdapter</code>","text":"<p>               Bases: <code>LLMAdapter</code></p> <p>Adapter for Anthropic API.</p> Source code in <code>src/django_ai_validator/llm/adapters.py</code> <pre><code>class AnthropicAdapter(LLMAdapter):\n    \"\"\"Adapter for Anthropic API.\"\"\"\n    def __init__(self, api_key: str = None, model: str = \"claude-3-opus-20240229\", **kwargs):\n        self.api_key = api_key or getattr(settings, 'ANTHROPIC_API_KEY', os.environ.get(\"ANTHROPIC_API_KEY\"))\n        self.model = model\n        try:\n            import anthropic\n            self.client = anthropic.Anthropic(api_key=self.api_key)\n        except ImportError:\n            raise ImportError(\"Anthropic package is not installed. Please install 'anthropic'.\")\n\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nRespond with 'VALID' if it meets the criteria. Otherwise, explain why it is invalid.\"\n        message = self.client.messages.create(\n            model=self.model,\n            max_tokens=1024,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        content = message.content[0].text.strip()\n        if content.upper().startswith(\"VALID\"):\n            return True, None\n        else:\n            return False, content\n\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nReturn ONLY the cleaned/normalized value.\"\n        message = self.client.messages.create(\n            model=self.model,\n            max_tokens=1024,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return message.content[0].text.strip()\n</code></pre>"},{"location":"api/#django_ai_validator.llm.adapters.GeminiAdapter","title":"<code>GeminiAdapter</code>","text":"<p>               Bases: <code>LLMAdapter</code></p> <p>Adapter for Google Gemini API.</p> Source code in <code>src/django_ai_validator/llm/adapters.py</code> <pre><code>class GeminiAdapter(LLMAdapter):\n    \"\"\"Adapter for Google Gemini API.\"\"\"\n    def __init__(self, api_key: str = None, model: str = \"gemini-pro\", **kwargs):\n        self.api_key = api_key or getattr(settings, 'GEMINI_API_KEY', os.environ.get(\"GEMINI_API_KEY\"))\n        self.model = model\n        try:\n            import google.generativeai as genai\n            genai.configure(api_key=self.api_key)\n            self.client = genai.GenerativeModel(self.model)\n        except ImportError:\n            raise ImportError(\"Google Generative AI package is not installed. Please install 'google-generativeai'.\")\n\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nRespond with 'VALID' if it meets the criteria. Otherwise, explain why it is invalid.\"\n        response = self.client.generate_content(prompt)\n        content = response.text.strip()\n        if content.upper().startswith(\"VALID\"):\n            return True, None\n        else:\n            return False, content\n\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nReturn ONLY the cleaned/normalized value.\"\n        response = self.client.generate_content(prompt)\n        return response.text.strip()\n</code></pre>"},{"location":"api/#django_ai_validator.llm.adapters.LLMAdapter","title":"<code>LLMAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Target interface for the Adapter Pattern. Standardizes interaction with different LLM providers.</p> Source code in <code>src/django_ai_validator/llm/adapters.py</code> <pre><code>class LLMAdapter(abc.ABC):\n    \"\"\"\n    Target interface for the Adapter Pattern.\n    Standardizes interaction with different LLM providers.\n    \"\"\"\n    @abc.abstractmethod\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        pass\n\n    @abc.abstractmethod\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        pass\n</code></pre>"},{"location":"api/#django_ai_validator.llm.adapters.OllamaAdapter","title":"<code>OllamaAdapter</code>","text":"<p>               Bases: <code>LLMAdapter</code></p> <p>Adapter for Ollama (Llama) API.</p> Source code in <code>src/django_ai_validator/llm/adapters.py</code> <pre><code>class OllamaAdapter(LLMAdapter):\n    \"\"\"Adapter for Ollama (Llama) API.\"\"\"\n    def __init__(self, host: str = None, model: str = \"llama3\", **kwargs):\n        self.host = host or getattr(settings, 'OLLAMA_HOST', os.environ.get(\"OLLAMA_HOST\"))\n        self.model = model\n        try:\n            import ollama\n            self.client = ollama.Client(host=self.host)\n        except ImportError:\n            raise ImportError(\"Ollama package is not installed. Please install 'ollama'.\")\n\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nRespond with 'VALID' if it meets the criteria. Otherwise, explain why it is invalid.\"\n        response = self.client.chat(model=self.model, messages=[\n            {'role': 'user', 'content': prompt},\n        ])\n        content = response['message']['content'].strip()\n        if content.upper().startswith(\"VALID\"):\n            return True, None\n        else:\n            return False, content\n\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nReturn ONLY the cleaned/normalized value.\"\n        response = self.client.chat(model=self.model, messages=[\n            {'role': 'user', 'content': prompt},\n        ])\n        return response['message']['content'].strip()\n</code></pre>"},{"location":"api/#django_ai_validator.llm.adapters.OpenAIAdapter","title":"<code>OpenAIAdapter</code>","text":"<p>               Bases: <code>LLMAdapter</code></p> <p>Adapter for OpenAI API.</p> Source code in <code>src/django_ai_validator/llm/adapters.py</code> <pre><code>class OpenAIAdapter(LLMAdapter):\n    \"\"\"Adapter for OpenAI API.\"\"\"\n    def __init__(self, api_key: str = None, model: str = \"gpt-3.5-turbo\", **kwargs):\n        self.api_key = api_key or getattr(settings, 'OPENAI_API_KEY', os.environ.get(\"OPENAI_API_KEY\"))\n        self.model = model\n        try:\n            from openai import OpenAI\n            self.client = OpenAI(api_key=self.api_key)\n        except ImportError:\n            raise ImportError(\"OpenAI package is not installed. Please install 'openai'.\")\n\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nRespond with 'VALID' if it meets the criteria. Otherwise, explain why it is invalid.\"\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful data validation assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.0,\n        )\n        content = response.choices[0].message.content.strip()\n        if content.upper().startswith(\"VALID\"):\n            return True, None\n        else:\n            return False, content\n\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        prompt = f\"{prompt_template}\\n\\nInput: {value}\\n\\nReturn ONLY the cleaned/normalized value.\"\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful data cleaning assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.0,\n        )\n        return response.choices[0].message.content.strip()\n</code></pre>"},{"location":"advanced/architecture/","title":"Architecture","text":"<p><code>django-ai-validator</code> is designed with modularity and extensibility in mind. It uses several design patterns to ensure flexibility and testability.</p>"},{"location":"advanced/architecture/#core-components","title":"Core Components","text":""},{"location":"advanced/architecture/#1-validators-validatorspy","title":"1. Validators (<code>validators.py</code>)","text":"<ul> <li>Pattern: Template Method</li> <li>Role: <code>BaseAIValidator</code> defines the validation workflow (prepare -&gt; validate -&gt; handle error). Subclasses like <code>AISemanticValidator</code> implement specific logic.</li> </ul>"},{"location":"advanced/architecture/#2-fields-fieldspy","title":"2. Fields (<code>fields.py</code>)","text":"<ul> <li>Role: <code>AICleanedField</code> is a custom model field that intercepts <code>pre_save</code> signals to clean data. It supports both synchronous and asynchronous modes.</li> </ul>"},{"location":"advanced/architecture/#3-facade-facadepy","title":"3. Facade (<code>facade.py</code>)","text":"<ul> <li>Pattern: Facade</li> <li>Role: <code>AICleaningFacade</code> provides a simplified interface for the rest of the system to interact with LLMs. It handles provider selection, caching, and error handling.</li> </ul>"},{"location":"advanced/architecture/#4-llm-adapters-llmadapterspy","title":"4. LLM Adapters (<code>llm/adapters.py</code>)","text":"<ul> <li>Pattern: Adapter</li> <li>Role: Standardizes the interface for different LLM providers (OpenAI, Anthropic, etc.). Each adapter implements <code>validate</code> and <code>clean</code> methods.</li> </ul>"},{"location":"advanced/architecture/#5-factory-llmfactorypy","title":"5. Factory (<code>llm/factory.py</code>)","text":"<ul> <li>Pattern: Abstract Factory / Registry</li> <li>Role: <code>LLMFactory</code> creates instances of adapters based on configuration. It allows registration of custom providers.</li> </ul>"},{"location":"advanced/architecture/#6-proxy-llmproxypy","title":"6. Proxy (<code>llm/proxy.py</code>)","text":"<ul> <li>Pattern: Proxy</li> <li>Role: <code>CachingLLMProxy</code> wraps adapters to add caching behavior without modifying the adapter code.</li> </ul>"},{"location":"advanced/architecture/#7-cache-manager-cachepy","title":"7. Cache Manager (<code>cache.py</code>)","text":"<ul> <li>Pattern: Singleton</li> <li>Role: <code>LLMCacheManager</code> handles generating cache keys and interacting with Django's cache framework.</li> </ul>"},{"location":"advanced/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Input: User submits data (e.g., via Form or Admin).</li> <li>Validator/Field: The validator or field intercepts the data.</li> <li>Facade: The request is passed to the <code>AICleaningFacade</code>.</li> <li>Factory: The facade asks the factory for the configured LLM adapter.</li> <li>Proxy: The factory returns a proxy (if caching is enabled) wrapping the adapter.</li> <li>Cache Check: The proxy checks if the result is in the cache.</li> <li>LLM Call: If not cached, the adapter calls the external LLM API.</li> <li>Result: The result is returned, cached, and passed back to the validator/field.</li> <li>Action: The validator raises <code>ValidationError</code> or the field updates the model instance.</li> </ol>"},{"location":"advanced/async/","title":"Asynchronous Cleaning","text":"<p>Asynchronous cleaning allows you to offload the time-consuming LLM API calls to a background worker. This ensures your application remains responsive.</p>"},{"location":"advanced/async/#prerequisites","title":"Prerequisites","text":"<p>You must have Celery installed and configured in your Django project.</p> <ol> <li>Install Celery: <code>pip install celery</code></li> <li>Configure Celery in your project (see Celery Django docs).</li> </ol>"},{"location":"advanced/async/#how-to-use","title":"How to Use","text":"<p>Simply set <code>use_async=True</code> on your <code>AICleanedField</code>.</p> <pre><code>content = AICleanedField(\n    cleaning_prompt=\"...\",\n    use_async=True\n)\n</code></pre>"},{"location":"advanced/async/#the-ai_clean_model_instance-task","title":"The <code>ai_clean_model_instance</code> Task","text":"<p>The library provides a shared task <code>django_ai_validator.tasks.ai_clean_model_instance</code>.</p> <p>When <code>use_async=True</code>, this task is called with: - <code>app_label</code> - <code>model_name</code> - <code>instance_id</code> - <code>field_name</code> - <code>prompt_template</code></p> <p>The task fetches the instance, calls the LLM, updates the field, and saves the instance.</p>"},{"location":"advanced/async/#considerations","title":"Considerations","text":"<ul> <li>Race Conditions: Be aware that the field will contain the \"dirty\" value until the task completes.</li> <li>Error Handling: If the task fails (e.g., API error), the field will remain dirty. You should monitor your Celery worker logs.</li> </ul>"},{"location":"advanced/configuration/","title":"Configuration","text":"<p>You can configure Django AI Validator in your <code>settings.py</code>.</p>"},{"location":"advanced/configuration/#default-provider","title":"Default Provider","text":"<p>Set the default LLM provider for the entire project:</p> <pre><code>AI_CLEANER_DEFAULT_PROVIDER = 'openai'  # 'anthropic', 'gemini', or 'ollama'\n</code></pre>"},{"location":"advanced/configuration/#api-keys","title":"API Keys","text":"<p>Ensure your API keys are set in your environment variables:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GEMINI_API_KEY</code> (for Google Gemini)</li> <li><code>OLLAMA_HOST</code> (optional, defaults to localhost:11434 for Ollama)</li> </ul>"},{"location":"advanced/configuration/#caching","title":"Caching","text":"<p>The library uses Django's cache framework to cache LLM responses. This saves money and time for repeated identical requests.</p> <p>To configure the cache timeout (in seconds):</p> <pre><code># settings.py\nAI_CLEANER_CACHE_TIMEOUT = 3600  # Default is 1 hour\n</code></pre>"},{"location":"advanced/configuration/#registering-custom-providers","title":"Registering Custom Providers","text":"<p>You can register your own LLM providers using the <code>LLMFactory</code>.</p> <pre><code># apps.py in your app\nfrom django.apps import AppConfig\nfrom django_ai_validator.llm.factory import LLMFactory\nfrom .my_custom_provider import MyCustomFactory\n\nclass MyAppConfig(AppConfig):\n    def ready(self):\n        LLMFactory.register('my_provider', MyCustomFactory)\n</code></pre>"},{"location":"advanced/custom-providers/","title":"Custom Providers","text":"<p><code>django-ai-validator</code> supports OpenAI, Anthropic, Gemini, and Ollama out of the box. However, you can easily add support for other LLM providers (e.g., Cohere, Azure OpenAI, local models) by implementing a custom adapter.</p>"},{"location":"advanced/custom-providers/#1-create-an-adapter","title":"1. Create an Adapter","text":"<p>Create a class that inherits from <code>django_ai_validator.llm.adapters.LLMAdapter</code> and implements the <code>validate</code> and <code>clean</code> methods.</p> <pre><code>from typing import Tuple, Optional\nfrom django_ai_validator.llm.adapters import LLMAdapter\n\nclass MyCustomAdapter(LLMAdapter):\n    def __init__(self, api_key=None, **kwargs):\n        self.api_key = api_key\n        # Initialize your client here\n\n    def validate(self, value: str, prompt_template: str) -&gt; Tuple[bool, Optional[str]]:\n        # Call your LLM API\n        # Return (True, None) if valid\n        # Return (False, \"Reason\") if invalid\n        pass\n\n    def clean(self, value: str, prompt_template: str) -&gt; str:\n        # Call your LLM API\n        # Return the cleaned string\n        pass\n</code></pre>"},{"location":"advanced/custom-providers/#2-create-a-factory","title":"2. Create a Factory","text":"<p>Create a factory class that returns an instance of your adapter.</p> <pre><code>class MyCustomFactory:\n    @staticmethod\n    def create_adapter(**kwargs):\n        return MyCustomAdapter(**kwargs)\n</code></pre>"},{"location":"advanced/custom-providers/#3-register-the-provider","title":"3. Register the Provider","text":"<p>Register your factory with the <code>LLMFactory</code> in your Django app's <code>ready</code> method.</p> <pre><code># apps.py\nfrom django.apps import AppConfig\nfrom django_ai_validator.llm.factory import LLMFactory\nfrom .adapters import MyCustomFactory\n\nclass MyAppConfig(AppConfig):\n    name = 'my_app'\n\n    def ready(self):\n        LLMFactory.register('my_provider', MyCustomFactory)\n</code></pre>"},{"location":"advanced/custom-providers/#4-use-the-provider","title":"4. Use the Provider","text":"<p>You can now use your custom provider in validators and fields.</p> <pre><code># settings.py\nAI_CLEANER_DEFAULT_PROVIDER = 'my_provider'\n\n# OR in a validator\nvalidator = AISemanticValidator(..., provider='my_provider')\n</code></pre>"},{"location":"user-guide/admin/","title":"Admin Integration","text":"<p>Django AI Validator provides tools to integrate AI capabilities directly into the Django Admin interface.</p>"},{"location":"user-guide/admin/#aiadminmixin","title":"AIAdminMixin","text":"<p>Use <code>AIAdminMixin</code> in your <code>ModelAdmin</code> to enable bulk cleaning actions.</p> <pre><code>from django.contrib import admin\nfrom django_ai_validator.admin import AIAdminMixin\nfrom .models import MyModel\n\n@admin.register(MyModel)\nclass MyModelAdmin(AIAdminMixin, admin.ModelAdmin):\n    list_display = ['content', 'is_dirty']\n    actions = ['run_ai_cleanup_on_selected']\n</code></pre>"},{"location":"user-guide/admin/#features","title":"Features","text":"<ul> <li><code>run_ai_cleanup_on_selected</code> Action: Select multiple rows and run the AI cleaning process on them. This is useful for batch processing existing data.</li> <li><code>is_dirty</code> Flag: If your model uses <code>AIDirtyMixin</code>, you can display the <code>is_dirty</code> status in the admin list view.</li> </ul>"},{"location":"user-guide/admin/#aidirtymixin","title":"AIDirtyMixin","text":"<p>Add <code>AIDirtyMixin</code> to your models to track whether they need cleaning.</p> <pre><code>from django_ai_validator.models import AIDirtyMixin\n\nclass MyModel(AIDirtyMixin, models.Model):\n    ...\n</code></pre> <p>This adds an <code>is_dirty</code> boolean field (default <code>True</code>). You can use this in your application logic to determine which records need processing.</p>"},{"location":"user-guide/best-practices/","title":"Best Practices","text":""},{"location":"user-guide/best-practices/#prompt-engineering","title":"Prompt Engineering","text":"<p>The quality of validation and cleaning depends heavily on your prompts.</p> <ul> <li>Be Specific: Clearly state what is allowed and what is not.</li> <li>Provide Examples: Few-shot prompting (giving examples in the prompt) can significantly improve accuracy.</li> <li>Use Delimiters: Use quotes or other delimiters to separate the input value from the instructions.</li> </ul> <p>Example:</p> <p>\"Check if the following text is a valid address. Return VALID if yes. Text: '{value}'\"</p>"},{"location":"user-guide/best-practices/#cost-management","title":"Cost Management","text":"<p>LLM APIs can be expensive.</p> <ul> <li>Use Caching: Enable caching (enabled by default) to avoid paying for the same validation twice.</li> <li>Use Cheaper Models: For simple tasks, use cheaper models like <code>gpt-3.5-turbo</code> or <code>gemini-pro</code> instead of <code>gpt-4</code>.</li> <li>Validate Locally First: Use standard Django validators (e.g., <code>MaxLengthValidator</code>, <code>EmailValidator</code>) to catch obvious errors before calling the LLM.</li> </ul> <pre><code>validators=[\n    MaxLengthValidator(500),  # Cheap check first\n    AISemanticValidator(...)  # Expensive check second\n]\n</code></pre>"},{"location":"user-guide/best-practices/#error-handling","title":"Error Handling","text":"<p>LLM APIs can fail (timeouts, rate limits).</p> <ul> <li>Synchronous: If the API fails during a synchronous save, a <code>ValidationError</code> or <code>ImportError</code> (if missing deps) might be raised. Ensure your UI handles these gracefully.</li> <li>Asynchronous: If using <code>AICleanedField(use_async=True)</code>, failures happen in the background. Monitor your Celery logs. The field will remain \"dirty\" if the task fails.</li> </ul>"},{"location":"user-guide/best-practices/#security","title":"Security","text":"<ul> <li>Prompt Injection: Be aware that malicious users might try to inject instructions into the input to manipulate the LLM.</li> <li>Sanitization: Always sanitize the output of the LLM if you plan to render it as HTML (though <code>AICleanedField</code> returns text, be careful how you use it).</li> </ul>"},{"location":"user-guide/fields/","title":"AICleanedField","text":"<p><code>AICleanedField</code> is a custom model field (inheriting from <code>TextField</code>) that automatically cleans or normalizes its content using an LLM.</p>"},{"location":"user-guide/fields/#synchronous-cleaning","title":"Synchronous Cleaning","text":"<p>By default, cleaning happens during the <code>pre_save</code> signal. This blocks the save operation until the LLM responds.</p> <pre><code>from django_ai_validator.fields import AICleanedField\n\nclass BlogPost(models.Model):\n    content = AICleanedField(\n        cleaning_prompt=\"Fix grammar, spelling, and improve clarity.\"\n    )\n</code></pre>"},{"location":"user-guide/fields/#asynchronous-cleaning","title":"Asynchronous Cleaning","text":"<p>For better performance, especially with large texts or slow LLM responses, you can enable asynchronous cleaning. This requires Celery.</p> <pre><code>class BlogPost(models.Model):\n    content = AICleanedField(\n        cleaning_prompt=\"Fix grammar, spelling, and improve clarity.\",\n        use_async=True\n    )\n</code></pre> <p>When <code>use_async=True</code>: 1. The model is saved immediately with the original \"dirty\" data. 2. A Celery task is triggered. 3. The task calls the LLM and updates the field with the cleaned version.</p>"},{"location":"user-guide/fields/#configuration","title":"Configuration","text":"<ul> <li><code>cleaning_prompt</code> (required): Instructions for the LLM on how to clean the data.</li> <li><code>use_async</code> (optional, default <code>False</code>): Whether to use a background task.</li> </ul>"},{"location":"user-guide/testing/","title":"Testing","text":"<p>Testing applications that use LLMs can be tricky and expensive if you hit the real API every time. <code>django-ai-validator</code> provides tools to make testing easier.</p>"},{"location":"user-guide/testing/#using-the-mock-provider","title":"Using the Mock Provider","text":"<p>The easiest way to test is to use the <code>mock</code> provider. This provider returns deterministic responses without making network requests.</p>"},{"location":"user-guide/testing/#configuration","title":"Configuration","text":"<p>In your <code>tests.py</code> or <code>conftest.py</code>, register the mock provider:</p> <pre><code>from django.test import TestCase, override_settings\nfrom django_ai_validator.llm.factory import LLMFactory\nfrom django_ai_validator.llm.mock_factory import MockFactory\n\n# Register the mock factory (do this once, e.g., in AppConfig.ready or setUpClass)\nLLMFactory.register('mock', MockFactory)\n\n@override_settings(AI_CLEANER_DEFAULT_PROVIDER='mock')\nclass MyModelTests(TestCase):\n    def test_validation(self):\n        # This will use the MockAdapter\n        ...\n</code></pre>"},{"location":"user-guide/testing/#mock-adapter-behavior","title":"Mock Adapter Behavior","text":"<p>The default <code>MockAdapter</code>: - Validation: Returns <code>False</code> if the input contains \"bad\", otherwise <code>True</code>. - Cleaning: Replaces \"dirty\" with \"clean\".</p> <p>You can customize this behavior by creating your own mock adapter and factory if needed.</p>"},{"location":"user-guide/testing/#mocking-with-unittestmock","title":"Mocking with <code>unittest.mock</code>","text":"<p>For more granular control, you can mock the <code>AICleaningFacade</code> or the underlying LLM client.</p> <pre><code>from unittest.mock import patch\nfrom django.test import TestCase\nfrom .models import MyModel\n\nclass MyTests(TestCase):\n    @patch('django_ai_validator.validators.AICleaningFacade.validate')\n    def test_custom_validation(self, mock_validate):\n        mock_validate.return_value = (True, None)\n\n        # Your test code here\n        ...\n</code></pre>"},{"location":"user-guide/testing/#testing-asynchronous-tasks","title":"Testing Asynchronous Tasks","text":"<p>If you are using <code>use_async=True</code>, you can test the task execution by using <code>celery.contrib.testing</code> or by mocking the task.</p> <pre><code>from unittest.mock import patch\n\n@patch('django_ai_validator.tasks.ai_clean_model_instance.delay')\ndef test_async_trigger(self, mock_delay):\n    # Save model\n    instance.save()\n\n    # Verify task was called\n    mock_delay.assert_called_once()\n</code></pre>"},{"location":"user-guide/validators/","title":"Validators","text":"<p>The <code>AISemanticValidator</code> allows you to perform complex semantic validation that traditional regex or logic-based validators cannot handle.</p>"},{"location":"user-guide/validators/#usage","title":"Usage","text":"<p>Import the validator and add it to your model or form field's <code>validators</code> list.</p> <pre><code>from django_ai_validator.validators import AISemanticValidator\n\nname = models.CharField(\n    max_length=100,\n    validators=[\n        AISemanticValidator(\n            prompt_template=\"Check if this is a valid full name (First Last). It should not be a company name.\"\n        )\n    ]\n)\n</code></pre>"},{"location":"user-guide/validators/#how-it-works","title":"How it Works","text":"<p>The validator sends the field's value along with your <code>prompt_template</code> to the configured LLM. The LLM is instructed to determine if the value meets the criteria.</p> <ul> <li>If valid, the LLM returns \"VALID\".</li> <li>If invalid, the LLM returns an explanation, which is raised as a <code>ValidationError</code>.</li> </ul>"},{"location":"user-guide/validators/#customizing-the-provider","title":"Customizing the Provider","text":"<p>You can specify a different LLM provider for a specific validator:</p> <pre><code>AISemanticValidator(\n    prompt_template=\"...\",\n    provider=\"anthropic\"  # Use Anthropic instead of the default\n)\n</code></pre>"}]}